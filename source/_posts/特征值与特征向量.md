---
title: 特征值与特征向量
date: 2018-07-05 20:14:02
tags: [线性代数, Machine learning]
toc: true
categories: [线性代数]
mathjax: true
description: 奇异值分解(Singular Value Decomposition，一下简称SVD)是在机器学习领域广泛应用的算法，它不光可以用于降维算法中的特征分解，还可以用于推荐系统，以及自然语言处理等领域。是很多机器学习算法的基石。这篇文章先简单描述奇异值分解的基础，特征值分解。
---

## 特征值和特征向量

### 定义
**特征值和特征向量的定义：** $$Ax = \lambda x$$ 其中$A$是$n*n$的矩阵，$x$是一个$n$维向量，我们说$\lambda$是矩阵$A$的一个特征值，而$x$是矩阵$A$的特征值$\lambda$对应的特征向量。

式$Ax = \lambda x$也可以写成$$(A - \lambda E)x = 0$$这是n个未知数n个方程的齐次线性方程组，它有解的充分不要条件是系数行列式$|A-\lambda E| = 0$其中$E$是单位矩阵。 

### 几何意义

主要参考：[知乎@马同学](https://www.zhihu.com/question/21874816)的回答

矩阵乘以一个向量的结果仍是同维数的一个向量，因此，**矩阵乘法对应了一个变换，把一个向量变成同维数的另一个向量**。

下面考虑二维情况： 

在基向量$\overrightarrow{i}$, $\overrightarrow{j}$下，有向量$\overrightarrow{v}$： 
![图1](https://pic4.zhimg.com/80/v2-9c697544d33a102f75e33dfc1fc2bcd3_hd.png)

对向量$\overrightarrow{v}$左乘矩阵$A$,得到结果如下($\overrightarrow{v}$变化为$A\overrightarrow{v} 方向和长度都发生了变化$)： 
![图2， 左乘矩阵$A$](https://pic4.zhimg.com/80/v2-72998d1d940aec9a4e79fd4548cfc32b_hd.png)

接下来我们调整一下$\overrightarrow {v}$的方向， 选取一个特殊的$\overrightarrow {v '}$，同样左乘矩阵$A$，向量$\overrightarrow{v'}$的变化如下： 
![图3， 特殊情况矩阵与特征向量相乘的情况](https://pic2.zhimg.com/80/v2-d1f9c14af565b0833c07e5f009c4022d_hd.png)

可以发现， 调整后的$\overrightarrow{v'}$和$A \overrightarrow{v'}$在同一根直线上，只是$A \overrightarrow{v'}$的长度相对$\overrightarrow {v'}$的长度变长了。此时我们就称$\overrightarrow {v'}$是$A$的特征向量，而$A \overrightarrow{v'}$的长度是$\overrightarrow{v'}$的$\lambda$倍，$\lambda$就是特征值，从而特征值与特征向量之间的关系： **$\overrightarrow{v'}$在$A$的作用下，保持方向不变，只进行比例为$\lambda$的伸缩**。

接下来我们回到利用矩阵对向量进行线性变换的问题， 使用矩阵$A$对向量$\overrightarrow{v}$进行多次的变换操作，可以观察到如下的效果： 
![图4， 连续的线性变换](https://pic2.zhimg.com/80/v2-61e8b943d50f007d9ceaae98dc347671_hd.png)

小结：**1. 矩阵的特征向量经过矩阵的线性变换（左乘矩阵）后，方向不变，只改变长度； 2. 使用矩阵对任意向量进行多次线性变换， 结果最终会越来越贴合到最大的特征值对应的特征空间上。**

### 特征值分解
如果我们求出了矩阵$A$的特征值$\lambda _1 \leq \lambda _2 \leq ... \leq \lambda _n$，以及这n个特征值所对应的特征向量$\{ w_1, w_2, ...,w_n \}$，那么矩阵$A$就可以用下式的特征分解表示$$A = W \Sigma W ^{-1}$$ 其中$W$是这个$n$个特征向量所构成的$n*n$维矩阵， 而$\Sigma$为这$n$个特征值为主对角线的$n*n$维矩阵。

一般我们会把$W$的这$n$个特征向量标准化，即满足$\|w\| _2 = 1$， 或者说$w_i ^T w_i = 1$, 此时$W$的n个特征向量为标准正交基，满足$W ^T W = I$,即$W^T = W ^{-1}$，也就是说$W$为酉矩阵。

这样我们的特征值分解表达式可以写成 $$A = W \Sigma W ^T$$

## 参考引用

1. [知乎@马同学](https://www.zhihu.com/question/21874816)
2. [知乎@达闻西](https://www.zhihu.com/question/30094611/answer/120499954)
3. [奇异值分解(SVD)原理与降维中的应用](https://www.cnblogs.com/pinard/p/6251584.html)
4. [维基百科-特征值和特征向量](https://zh.wikipedia.org/zh-cn/%E7%89%B9%E5%BE%81%E5%80%BC%E5%92%8C%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F#%E4%B9%A6%E7%B1%8D)
