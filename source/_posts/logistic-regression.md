---
title: 逻辑回归小记
date: 2018-06-19 19:54:57
tags: [逻辑回归, Machine learning]
toc: true
categories: [Machine learning]
mathjax: true
description: 逻辑斯蒂回归(logistic regression)是统计学习中的经典分类方法。
---

### 预备知识

#### 分布函数

设$X$是一个随机变量， $x$是任意实数，函数$F(x) = p(X \leqslant x)$成为X的分布函数，有时也记为$X \sim F(x)$。 

对任意实数$x_1, x_2 (x_1 < x_2)$ 有 $$P(x_1 < X \leqslant x_2) = P(x \leqslant x_2) - P(x \leqslant x_1) = F(x_2) - F(x_1)$$ 因此，若已知$X$的分布函数，就可以知道$X$落在任意空间上的概率，在这个意义上说，分布函数完整地描述了随机变量的统计规律性。

#### 似然函数与极大似然估计

在数理统计学中，似然函数是一种关于统计模型中的参数的函数，表示模型参数中的似然性。似然函数在统计推断中有重大作用，如在极大似然估计和费雪信息之中的应用等等。“似然性”与“或然性”或“概率”意思相近，都是值某种事件发生的可能性，但是在统计学中，“似然性”和“或然性”或“概率”又有明确的区分。概率用于在已知一些参数的情况下，预测接下来的观测所得到的结果，二似然性则是用于在已知某些观测所得到的结果时，对有关事务的性质的参数进行估计。 

在这种意义上，似然函数 可以理解为条件概率的逆反。在一直某个参数$B$时，事件$A$发生的概率写作：$$P(A|B) = \frac {P(A,B)} {P(B)} $$ 利用贝叶斯定理 $$ P(B | A) = \frac {P(A|B) P(B)} {P(A)}$$ 因此我们可以反过来构造表示似然性的方法： 一直有时间$A$发生，运用似然函数$\Bbb{L} (B | A)$,我们估计参数B的可能性。形式上，似然函数也是一种条件概率函数，但我们关注的变量改变了： $$b \mapsto P(A | B = b)$$ 注意这里并不要求似然函数满足归一性 $\sum _{b \in \mathcal{B}} P(A | B = b) = 1$。 一个似然该函数诚意一个正的常数之后仍然是似然函数，对所有 $\alpha > 0$，都可以有似然函数： $$L(b | A) = \alpha P(A | B = b)$$

具体例子参照 [维基百科](https://zh.wikipedia.org/zh-cn/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0#%E4%BE%8B%E5%AD%90)

##### 极大似然估计

最大似然估计是似然函数最初也是最自然的应用。 上文已经提到，似然函数去的最大值表示相应的参数能够使得统计模型最为合理。 从这样一个想法出发，最大似然估计的做法是：首先选取似然函数（一般是概率密度函数或概率质量函数），整理之后求最大值。实际应用中一般会取似然函数的对数作为求取最大值的函数，这样求出的最大值和直接求最大值得到的结果是相同的。似然函数的最大值不一定唯一，也不一定存在。与矩法估计比较，最大似然估计的精确度较高，信息损失较少，但计算量大。 

### 逻辑斯蒂分布

**定义1 （逻辑斯蒂分布）** 设$X$是连续随机变量， $X$服从逻辑斯蒂分布是指$X$具有下列分布函数和密度函数：$$F(x) = P(X \leqslant x) = \frac {1} {1 + e ^{-(x - \mu) / \gamma }}$$ $$f(x) = F'(x) = \frac {e ^{-(x-\mu) / \gamma}} {\gamma (1 + e ^{-(x-\mu) / \gamma}) ^2}$$

式中，$\mu$为位置参数， $\gamma > 0$为形状参数。

逻辑斯蒂分布的密度函数$f(x)$和分布函数$F(x)$的图形如图(略)。分布函数属于逻辑斯蒂函数，其图形是一条S型曲线(sigmoid curve)。该曲线以点$\left( \mu, \frac {1} {2} \right)$为中心对称，即满足$$F(-x + \mu) - \frac {1} {2} = -F(x + \mu) + \frac {1} {2}$$ 曲线在中心附近增长较快，在两端增长速度较慢。形状参数$\gamma$的值越小，曲线在中心附近增长的越快。

### 二项逻辑斯蒂回归模型

二项逻辑斯蒂回归模型(binomial logistic regression model)是一种分类模型，由条件概率分布$P(Y|X)$表示，形式为参数化的逻辑斯蒂分布。这里，随机变量$X$取值为实数，随机变量$Y$取值为1或0。我们通过监督学习的方法来估计模型参数。

**定义2 （逻辑斯蒂回归模型）** 二项逻辑斯蒂回归模型是如下的条件概率分布：$$P(Y = 1| x) = \frac{exp(w \cdotp x + b)} {1 + exp (w \cdotp x + b)} \tag{1}$$ $$P(Y=0 | x) = \frac {1} {1+exp (w \cdotp x + b)} \tag{2}$$ 这里， $x \in R ^n$是输入， $Y \in \{0, 1\}$是输出，$w \in R ^n$和$b \in R$是参数， $w$成为权值向量， $b$称为偏置。 $w \cdotp x$为$w$和$x$的内积。

对于给定的输入实例$x$, 按照式1和式2可以求得$P(Y=1 | x)$和$P(Y=0 | x)$。逻辑斯蒂回归模型比较两个概率值的大小，将实例$x$分到概率值比较大的那一类。

有时为了方便，将权值向量和输入向量加以扩充，仍记作$w$, $x$, 即$w = (w^{(1)}, w^{(2)}, ..., w^{(n)}, b)^T$, $x = (x^{(1)}, x^{(2)}, ..., x^{(n)}, 1)$这时逻辑斯蒂回归模型如下： $$P(Y = 1| x) = \frac{exp(w \cdotp x)} {1 + exp (w \cdotp x)} \tag{3}$$ $$P(Y=0 | x) = \frac {1} {1+exp (w \cdotp x)} \tag{4}$$

现在考察逻辑斯蒂回归模型的特点。一个事件的几率(odds)是指该事件发生的概率与该事件不发生的概率的比值。如果该事件发生的概率是$p$,那么该事件的几率是$\frac {p} {1-p}$,该事件发生的对数几率（log odds）或logit函数是$$logit(p) = \log \frac {p} {1-p}$$, 对逻辑回归而言，由式3和式4得$$\log \frac {P(Y=1 | x)} {1 - P(Y=1 | x)} = w \cdotp x$$ 这就是说，在逻辑斯蒂回归模型中，输出$Y = 1$ 的对数几率是输入$x$的线性函数，或者说，输出$Y=1$的对数几率是由输入$x$的线性函数表示模型，即逻辑斯蒂回归模型。

换一个角度看，考虑对输入$x$进行分类的线性函数$w \cdotp x$，其值域为实数域，注意，这里$x \in R ^{n + 1}, \quad w \in R ^{n + 1}$ 通过逻辑斯蒂回归模型定义式3可以将线性函数$w \cdotp x$转换为概率： $$P(Y = 1 | x) = \frac{exp(w \cdotp x)} {1 + exp (w \cdotp x)}$$ 这时，线性函数的值越接近正无穷，概率值就越接近1； 线性函数的值越接近负无穷，概率的值就越接近0.这样的模型就是逻辑斯蒂回归模型。

### 模型参数估计

逻辑斯蒂回归模型学习时，对给定的训练数据集$T = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}$ 其中 $x_i \in R ^n$, $y \in \{0, 1\}$, 可以用极大似然估计法估计模型参数，从而得到逻辑斯蒂回归模型。 

设 : $$P(Y = 1| x) = \pi (x), \quad P(Y = 0 | x) = 1 - \pi(x) $$ 似然函数为 $$\prod _{i=1} ^N [\pi (x_i)] ^{y_i} [1 - \pi (x_i)] ^{1-y_i}$$ 对数似然函数为 
$$\begin{align} L(w) & = \sum _{i=1} ^N [y_i \log \pi (x_i) + (1-y_i)log(1-\pi (x_i))] \\ 
 & = \sum _{i=1} ^N \left[ y_i \log \frac {\pi (x_i)} {1- \pi(x_i)} + \log (1 - \pi (x_i)) \right] \\
 & = \sum _{i=1} ^N [y_i (w \cdotp x_i) - \log (1+ exp (w \cdotp x_i))] \end{align}$$ 对 $L(w)$ 求极大值，得到 $w$的估计值。 

这样，问题就变成了以对数似然函数为目标函数的最优化问题。逻辑斯蒂回归学习中通常采用的方法是梯度下降法及拟牛顿法。 

假设$w$的极大似然估计值是$\hat {w}$那么，学到的逻辑回归模型为 $$P(Y = 1| x) = \frac{exp( \hat {w} \cdotp x)} {1 + exp (\hat {w} \cdotp x)}$$ $$P(Y = 0| x) = \frac{1} {1 + exp (\hat {w} \cdotp x)}$$ 

### 参考文献

- [1] [维基百科](https://zh.wikipedia.org/zh-cn/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0)
- [2] [《统计学习方法》(李航)](https://book.douban.com/subject/10590856/)
- [3] [详解最大似然估计（MLE）、最大后验概率估计（MAP），以及贝叶斯公式的理解- CSDN 博客](https://blog.csdn.net/u011508640/article/details/72815981)
