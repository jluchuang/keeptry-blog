---
title: 矩阵分解在推荐系统中的应用(2)_SVD背后的模型
date: 2018-07-14 19:59:06
tags: [Machine learning]
toc: true
categories: [Machine learning]
mathjax: true
description: 矩阵分解在推荐系统中的应用翻译系列第二篇， SVD背后原理。 
---

**前言** 这是“矩阵分解在推荐系统中的应用”系列的第2篇翻译文章，英文原文请参考[nicolas-hug.com](http://nicolas-hug.com/blog/matrix_facto_2)。 

## 稠密矩阵SVD分解

这里让我们简单回顾一下第一部分： 

- 在输入矩阵$R$上做PCA之后，我们回得到用户的特征构成。这些用户的特征构成是用向量表示的(用户特征向量的长度和输入的每一行用户向量的长度相同，就像我们在做图片PCA过程中的特征脸和原始图片向量长度相同)。 因为这些特征构成是向量， 我们可以把这些向量放到一起构成矩阵， 记为$U$。 
- 在输入矩阵的转置矩阵上做PCA之后， 我们将会获得电影的特征构成。 这些电影的特征构成同样也用向量表示(长度同样和转置后的每一行Movie的长度相同)， 把电影的特征向量构成的矩阵记为$M$

### 矩阵分解

SVD到底能够产生什么效果？ 一个直观的印象是， SVD是在$R$和$R^{\top}$上做PCA。 

在输入矩阵$R$上做SVD将会产生两个矩阵$U$和$M$。分别是用户的特征构成和电影的特征构成。SVD通过将输入矩阵$R$特征分解成三个矩阵，进而输出$U$$M$,下面就是矩阵分解的具体形式：
$$R = M \Sigma U ^{\top}$$
其中，

- $R$等价于$M \Sigma U ^{\top}$
- 矩阵$M$可以复原输入矩阵$R$的所有列
- 矩阵$U$可以复原输入矩阵$R$的所有行
- 矩阵$M$的每一列都是正交的(orthogonal)， 同样矩阵$U$的每一列也都是正交的，这一点是之前没有被提到的，矩阵的主成分总是正交的。这是PCA和SVD的重要性质，但是对于推荐的应用场景来说，我们并不关心这一点。 
- $\Sigma$是一个对角矩阵（后面会谈到这一点）

### SVD背后的模型

当我们采取SVD分解的方式对输入的打分矩阵$R$的位置评分机型计算的时候，这其实是一种非常具体并且有意义的计算方法。接下来我们描述模型。 

为了描述简单，我们忽略对角矩阵$\Sigma$: 它本身是一个对角矩阵，因此它实际上起到的作用是在$M$$U^{\top}$的基础上做伸缩的作用。因此这里我们把之前的公式简化成$$R = MU^{\top}$$

现在， 在矩阵分解的基础上， 我们考虑用户$u$对item$i$的打分，我们用$r_{ui}$来表示：
$$ 
 \begin{pmatrix} r_{ui} \end{pmatrix} =
 \begin{pmatrix} \cdots & p_u & \cdots \end{pmatrix}
 \begin{pmatrix} \vdots \\  & q_i & \\ \vdots \end{pmatrix} 
 $$

可以看到最终的$r_{ui}$是矩阵$U$中代表用户$u$的特征构成向量$p_u$和矩阵M中代表电影$i$的特征构成向量$q_i$之间的内积：$$r_{ui} = p_u \cdot q_i$$


