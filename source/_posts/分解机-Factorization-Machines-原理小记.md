---
title: 分解机(Factorization Machines)原理小记
date: 2018-07-08 15:44:18
tags: [线性代数, Machine learning]
toc: true
categories: [线性代数]
mathjax: true
description: FM算法，全称Factorization Machines, 一般翻译为“因子分解机”。 2010年，它由当时还在日本大阪大学的Steffen Rendle提出。此算法的主要作用是可以把所有特征进行高阶组合，减少人工参与特征组合的工作，工程师可以将经历集中在模型参数调优。FM只需要线性时间复杂度，可以应用于大规模机器学习。经过部分数据集试验，此算法在稀疏数据集上的效果要明显好于SVM。
---

FM算法，全称Factorization Machines, 一般翻译为“因子分解机”。 2010年，它由当时还在日本大阪大学的Steffen Rendle提出。此算法的主要作用是可以把所有特征进行高阶组合，减少人工参与特征组合的工作，工程师可以将经历集中在模型参数调优。FM只需要线性时间复杂度，可以应用于大规模机器学习。经过部分数据集试验，此算法在稀疏数据集上的效果要明显好于SVM。

## 预测任务

在机器学习中，预测(prediction)是一项最基本的任务。所谓的预测就是估计一个函数$$\hat{y}:\Bbb{R} ^n \rightarrow T \tag{1.1}$$该函数将一个$n$维的是指特征向量$x \in \Bbb{R}^n$映射到一个目标域$T$，例如对于回归$T=\Bbb{R}$对于分类问题$T=\{1, -1\}$在监督学习场景中，通常还有一个带标签的训练数据集$$D=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(N)}, y^{(N)}) \} \tag{1.2}$$其中$x^{(i)} \in \Bbb{R} ^n$表示输入数据，对应样本的特征向量，$y^{(i)}$对应标签，$N$为样本的数目。

在现实世界中，许多应用问题(如文本分析、推荐系统等)会产生高度稀疏的（特征）数据，即特征向量$x^{(i)}$中几乎所有的分量都为0.这里以电影评分系统为例，给出一个关于高度稀疏数据的实例。 

* 例1.1 * 考虑电影系统中的数据，它的每一条记录都包含了哪个用户$u \in U$在什么时候 $t \in \Bbb{R}$对哪部电影$i \in I$打了多少分$r \in \{1, 2, 3, 4, 5\}$这样的信息，假定用户集$U$和电影集$I$分别为$$U = \{Alice(A), Bob(B), Charlie(C), ...\}$$ $$I=\{Titanic(TI), Notting Hill(NH), Star Wars(SW), Star Trek(ST), ...\}$$设观测数据集为$$S=\{(A, TI, 2010-1, 5), (A, NH, 2010-2, 3), (C, TI, 2009-9, 1) ...\}$$

利用观测数据集$S$来进行预测任务的一个实例是：估计一个函数$\hat{y}$来预测某个用户在某个时刻对某部电影的打分行为。

有了观测数据$S$，如何构造形如$(1.2)$那样的样本数据呢？图1给出了利用$S$构造特征向量和标签的示例。

![图1 利用观测数据 S 构造特征向量和标签的示例](https://wx2.sinaimg.cn/mw690/7c35df9bly1ft2j3mmnefj20z60g2n64.jpg)

图1中的标签部分比较简单，直接将当前用户对当前电影的评分作为标签，如第一条观测记录中，Alice对TI的评分是5，则有$y ^{(1)} = 5$, 而特征向量$x$由以下5部分构成：

1. 第一部分（对应蓝色方框）表征*当前评分用户信息*，其维度为$|U|$，该部分的分量中，当前电影评分用户所在的位置为1，其它为0，例如在第一条观测记录中就有$x_A^{(1)} = 1$。
2. 第二部分（对应红色方框）表征*当前被评分电影的信息*，其维度为$|I|$，该部分的分量中，当前被评分的电影所在的位置为1，其他为0.例如第一条观测记录中就有$x_{TI} ^{(1)} = 1$。
3. 第三部分（对应黄色方框）表征*当前评分用户评分过的所有电影信息*，其维度为|I|，该部分的分量中，被当前用户评论过的所有电影（设个数为$n_I$）的位置为$\frac{1}{n_I}$,其他为0.例如Alice评价过散步电影TI,NH,和SW，因此就有$x^{(1)}_{TI} = x^{(1)}_{NH} = x^{(1)}_{SW} = 1/3$。 
4. 第四部分（对应绿色方框）表征*评分日期信息*，其维度为1，用来表示用户评价电影的时间，表示方法是将（记录中最早的日期）2009年1月作为基数1每增加一个月就加1，例如2009年5月就为5. 
5. 第五部分（对应棕色方框）表征*当前评分用户最近评分过的一部电影的信息*，其维度为$|I|$，该部分的分量中，若当前用户评价当前电影之前还评价过其他电影，则将当前用户评价的上一步电影的位置取为1其他为0；若当前用户评价当前电影之前没有评价过其他电影，则所有分量都取为0.例如，对于第二条观测记录，Alice评价NH之前评价的是TI,因此有$x_{TI} ^{(2)} = 1$。

*注1.1 上述描述中，x的上标表示观测记录的编号，下标表示分量的位置，为简单起见，这里通过颜色和符号来进行区分，而实际应用中，会将$x$的所有分量进行连续编号。*

在本例中，特征向量$x$的总维度为$|U| + |I| + |I| + 1 + |I| = |U| + 3|I| + 1$。在一个真是的电影评分系统中，用户数目|U|和电影的数目|I|是很大的，而每个用户参与评论的电影数目则相对很小，于是，可想而知，每条记录对应的额特征向量将会多么的稀疏。为定量刻画这种稀疏性，记$N_z(x)$表示$x$中非零向量的个数，并记$$\overline{N_z}(X) = \frac {1} {N} \sum _{i=1} ^N N_z(x^{(i)})$$其中$X$表示所有样本按行拼成的矩阵(Design Matrix)，即
$$ 
   \begin{pmatrix} 
   x_1^{(1)} & x_2^{(1)} & \cdots & x_n^{(1)} \\  
   x_1^{(2)} & x_2^{(2)} & \cdots & x_n^{(2)} \\  
   \vdots & \vdots  & \ddots & \vdots \\
   x_1^{(N)} & x_2^{(N)} & \cdots & x_n^{(N)} \\  
   \end{pmatrix}
$$
则$\overline{N_z}(X)$表征了训练集$D$中所有特征向量中非零分量的数的平均值。显然对于稀疏数据，成立$\overline{N_z}(X) << n$

## 模型方程

### 二阶表达式

在介绍FM的模型方程之前，先回顾一下线性回归。对于一个给定的特征向量$x = (x_1, x_2, ..., x_n)^{\top}$，线性回归建模时采用的函数是$$\begin{align} \hat{y}(x) &= w_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n \\ &= w_0 + \sum _{i = 1} ^n w_i x_i \end{align} \tag{2.3}$$其中，$w_0$和$\bf{w} = \{w_1, w_2, ..., w_n \} ^{\top}$为模型参数。从方程易见：各特征分量$x_i$和$x_j(i \neq j)$之间是相互独立的，即$\hat{y}(x)$中仅考虑单个的特征分量，而没有考虑特征分量之间的相互关系(interaction)。 

接下来我们在$(2.3)$的基础上，将函数$\hat{y}$改写为$$\hat{y}(x) = w_0 + \sum _{i = 1} ^n w_i x_i + \sum _{i=1} ^{n-1} \sum _{j=i+1} ^n w_{ij} x_i x_j \tag{2.4}$$这样，便将任意两个（互异）特征分量之间的关系也考虑进来了。

不过遗憾的是，这种直接在$x_i x_j$前面配上一个系数$w_{ij}$的方式在稀疏矩阵中有一个很大的缺陷，我们通过举例来进行说明。 

仍然考虑前面的例$(1.1)$，观测数据集$S$中没有Alice评价电影Star Trek的记录，如果要直接估计Alice(A)和Star Trek（ST）之间，或者说特征分量$x_A$和$x_{ST}$之间的相互关系，显然会得到系数$w_{A,ST} = 0$，即**对于观察样本中未出现过交互的特征分量，不能对相应的参数进行估计**。注意，在高度系数数据场景中，由于数量的不足，样本中出现未交互的特征分量是很普遍的。 

为了克服这个缺陷，我们在$(2.4)$中的系数$w_{ij}$上做文章，将其表成另外的形式。为此，针对每个维度的特征分量$x_i$引入辅助向量$$v_i = (v_{i1}, v_{i2}, ..., v_{ik}) ^{\top} \in \Bbb{R} ^k, i = 1,2, ..., n$$其中$k \in \Bbb {N} ^+$为超参数，并将$w_{ij}$改写为$$\hat{w}_ij = v_i ^{\top} v_j := \sum _{l=1} ^k v_{il}v_{jl} \tag{2.5}$$

这样就能克服上面提到的缺陷了么？我们粗略的分析一下。在观测数据中，Bob(B)对Start Wars(SW)和Star Trek(ST)的评分差不多。由此可认为他们对应的辅助向量$v_{ST}$和$v_{SW}$也比较相似。从而，利用$\hat{w}_{A,SW} = v_A ^{\top} v_{SW}$就可以对$\hat{w}_{A,ST} = v_A ^{\top} v_{ST}$进行估计了。（当然这并不是说，真的要先计算出$\hat{w}_{A,SW}$然后用它作为$\hat{w}_{A,ST}$的近似。因为这些性质都是观测数据中内在的，我们要做的只是建好模型，让模型在训练阶段自动去学习。）

于是，函数$\hat{y}$可以进一步写成$$\hat{y}(x) = w_0 + \sum _{i = 1} ^n w_i x_i + \sum _{i=1} ^{n-1} \sum _{j=i+1} ^n (v_i^{\top} v_j) x_i x_j \tag{2.6}$$

从数学的角度来看，$(2.4)$和$(2.6)$的主要区别在哪呢？对于交叉项$x_i x_j$的系数，前者用的是$w_{ij}$，后者用的是$\hat{w}_{ij}$为更好的看清楚他们之间的关系，引入几个矩阵：

- 将$\{v_i\}_{i=1}^{n}$按行拼接成长方$V$ 
$$ V = 
   \begin{pmatrix} 
   v_{11} & v_{12} & \cdots & v_{1k} \\  
   v_{21} & v_{22} & \cdots & v_{2k} \\  
   \vdots & \vdots  & \ddots & \vdots \\
   v_{n1} & v_{n2} & \cdots & v_{nk} \\  
   \end{pmatrix}
   = 
   \begin{pmatrix}
   v_1^{\top} \\ 
   v_2^{\top} \\ 
   \vdots\\
   v_n^{\top} \\ 
   \end{pmatrix}
   \tag{2.7}
$$

- 交互矩阵（interaction matrix）W
$$ W = 
   \begin{pmatrix} 
   w_{11} & w_{12} & \cdots & w_{1n} \\  
   w_{21} & w_{22} & \cdots & w_{2n} \\  
   \vdots & \vdots  & \ddots & \vdots \\
   w_{n1} & w_{n2} & \cdots & w_{nn} \\  
   \end{pmatrix}
   \tag{2.8}
$$

- 交互矩阵$\hat{W}$
$$ 
\begin{align}
\hat{W} & = VV^{\top} = 
   \begin{pmatrix}
     v_1^{\top} \\ 
     v_2^{\top} \\ 
     \vdots\\
     v_n^{\top} \\ 
   \end{pmatrix}
   (v_1, v_2, ..., v_n) \\
   & = 
   \begin{pmatrix}
   v^{\top}_1 v_1 & v^{\top}_1 v_2 & \cdots & v^{\top}_1 v_n \\  
   v^{\top}_2 v_1 & v^{\top}_2 v_2 & \cdots & v^{\top}_2 v_n \\  
   \vdots & \vdots  & \ddots & \vdots \\
   v^{\top}_n v_1 & v^{\top}_n v_2 & \cdots & v^{\top}_n v_n \\  
   \end{pmatrix} \\
   & = 
   \begin{pmatrix}
   v^{\top}_1 v_1 & \hat{w}_{12} & \cdots & \hat{w}_{1n} \\  
   \hat{w}_{21} & v^{\top}_2 v_2 & \cdots & \hat{w}_{2n} \\  
   \vdots & \vdots  & \ddots & \vdots \\
   \hat{w}_{n1} & \hat{w}_{n2} & \cdots & v^{\top}_n v_n \\  
   \end{pmatrix} \\
\end{align}
\tag{2.9}
$$

由此可见，$(2.4)$和$(2.6)$分别采用了交互矩阵$W$和$\hat{W}$的非对角线元素作为$x_i x_j$的系数。由于$\hat{W} = V V^{\top}$对应一种矩阵分解，因此我们将这种以$(2.6)$作为模型方程的方法称为Factorization Machines方法。

读着可能要问了，$VV^{\top}$的表达能力（expressiveness）怎么样呢？即任意给定一个交互矩阵$\hat{W}$，能否找到相应的分解矩阵$V$呢？答案是肯定的，这里需要用到一个结论“**当$k$足够大时，对任意对称正定的实矩阵$\hat{W} \in \Bbb{R}^{n*n}$均存在实矩阵$V \in \Bbb{R}^{n*k}$，使得成立$\hat{W} = VV^{\top}$。**”

## 参考引用

- [1] [Factorization Machines](http://www.algo.uni-konstanz.de/members/rendle/pdf/Rendle2010FM.pdf)
- [2] [一数一世界——“因子分解机FM-高效的组合高阶特征模型”](http://bourneli.github.io/ml/fm/2017/07/02/fm-remove-combine-features-by-yourself.html)
- [3] [分解机（Factorization Machines）推荐算法原理](https://www.cnblogs.com/pinard/p/6370127.html)

